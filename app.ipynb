{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Processamento da mão\u001b[39;00m\n\u001b[0;32m     44\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 45\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m output_frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n",
      "File \u001b[1;32mc:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ===== 1. Carregar o Modelo =====\n",
    "model = load_model('models/model (1).h5')  # Verifique o caminho do modelo\n",
    "\n",
    "# ===== 2. Classes =====\n",
    "class_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'I', 'L', 'M', \n",
    "                'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y']\n",
    "\n",
    "# ===== 3. Configurações do MediaPipe =====\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,  # Reduzido para melhor detecção\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# ===== 4. Configurações de Visualização =====\n",
    "BAR_WIDTH = 400\n",
    "BAR_HEIGHT = 480\n",
    "TEXT_OFFSET = 25  # Aumentado para melhor legibilidade\n",
    "FONT_SCALE = 0.6\n",
    "BAR_SPACING = 2\n",
    "\n",
    "# ===== 5. Captura de Vídeo =====\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Variáveis de estado\n",
    "    probabilities = None\n",
    "    predicted_class_index = -1\n",
    "    has_hand = False\n",
    "\n",
    "    # Processamento da mão\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    output_frame = frame.copy()\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        has_hand = True\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Desenho dos landmarks\n",
    "            mp_drawing.draw_landmarks(output_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Cálculo da ROI\n",
    "            h, w = frame.shape[:2]\n",
    "            x_coords = [int(lm.x * w) for lm in hand_landmarks.landmark]\n",
    "            y_coords = [int(lm.y * h) for lm in hand_landmarks.landmark]\n",
    "            \n",
    "            x_min, x_max = max(min(x_coords)-20, 0), min(max(x_coords)+20, w)\n",
    "            y_min, y_max = max(min(y_coords)-20, 0), min(max(y_coords)+20, h)\n",
    "            \n",
    "            # Extrair e processar ROI\n",
    "            hand_roi = frame[y_min:y_max, x_min:x_max]\n",
    "            if hand_roi.size == 0:\n",
    "                continue\n",
    "                \n",
    "            # Pré-processamento (ajustar conforme o modelo foi treinado)\n",
    "            processed_roi = cv2.resize(hand_roi, (64, 64))\n",
    "            processed_roi = cv2.cvtColor(processed_roi, cv2.COLOR_BGR2RGB)\n",
    "            processed_roi = processed_roi.astype(np.float32) / 255.0\n",
    "            processed_roi = np.expand_dims(processed_roi, axis=0)\n",
    "            \n",
    "            # Predição\n",
    "            predictions = model.predict(processed_roi, verbose=0)[0]\n",
    "            probabilities = predictions\n",
    "            predicted_class_index = np.argmax(probabilities)\n",
    "            \n",
    "            # Desenho da caixa\n",
    "            cv2.rectangle(output_frame, (x_min, y_min), (x_max, y_max), (0,255,0), 2)\n",
    "            cv2.putText(output_frame, f'Classe: {class_labels[predicted_class_index]}', \n",
    "                        (x_min, y_min-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "\n",
    "    # ===== 6. Renderização das Barras =====\n",
    "    bar_image = np.zeros((BAR_HEIGHT, BAR_WIDTH, 3), dtype=np.uint8)\n",
    "    \n",
    "    if has_hand and probabilities is not None:\n",
    "        # Calcular dimensões das barras\n",
    "        num_classes = len(class_labels)\n",
    "        class_height = BAR_HEIGHT // num_classes\n",
    "        max_bar_width = BAR_WIDTH - 20  # Margem para texto\n",
    "        \n",
    "        for i, (label, prob) in enumerate(zip(class_labels, probabilities)):\n",
    "            # Configurar posição\n",
    "            y_start = i * class_height + BAR_SPACING\n",
    "            y_end = (i+1) * class_height - BAR_SPACING\n",
    "            \n",
    "            # Calcular comprimento da barra\n",
    "            bar_length = int(prob * max_bar_width)\n",
    "            \n",
    "            # Cor e texto\n",
    "            color = (0, 200, 0) if i != predicted_class_index else (0, 0, 255)\n",
    "            text = f\"{label}: {prob*100:.1f}%\"\n",
    "            \n",
    "            # Desenhar elementos\n",
    "            cv2.rectangle(bar_image, (0, y_start), (bar_length, y_end), color, -1)\n",
    "            cv2.putText(bar_image, text, (10, y_start + TEXT_OFFSET),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, FONT_SCALE, (255,255,255), 2)\n",
    "    else:\n",
    "        # Mensagem quando nenhuma mão é detectada\n",
    "        text = \"Nenhuma mao detectada\"\n",
    "        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]\n",
    "        text_x = (BAR_WIDTH - text_size[0]) // 2\n",
    "        text_y = (BAR_HEIGHT - text_size[1]) // 2\n",
    "        cv2.putText(bar_image, text, (text_x, text_y),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "\n",
    "    # Combinar e exibir\n",
    "    combined = np.hstack((output_frame, bar_image))\n",
    "    cv2.imshow('Classificacao de Mãos', combined)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\marcc\\AppData\\Local\\Temp\\ipykernel_13060\\2474264056.py\", line 27, in classificar_imagem\n",
      "    model = load_model(model_path)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 196, in load_model\n",
      "    return legacy_h5_format.load_model_from_hdf5(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 116, in load_model_from_hdf5\n",
      "    f = h5py.File(filepath, mode=\"r\")\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 561, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py\\\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'model (1).h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\marcc\\AppData\\Local\\Temp\\ipykernel_13060\\2474264056.py\", line 27, in classificar_imagem\n",
      "    model = load_model(model_path)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 196, in load_model\n",
      "    return legacy_h5_format.load_model_from_hdf5(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 116, in load_model_from_hdf5\n",
      "    f = h5py.File(filepath, mode=\"r\")\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 561, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py\\\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'model (1).h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\marcc\\AppData\\Local\\Temp\\ipykernel_13060\\2474264056.py\", line 27, in classificar_imagem\n",
      "    model = load_model(model_path)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 196, in load_model\n",
      "    return legacy_h5_format.load_model_from_hdf5(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 116, in load_model_from_hdf5\n",
      "    f = h5py.File(filepath, mode=\"r\")\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 561, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py\\\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'model (1).h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\marcc\\AppData\\Local\\Temp\\ipykernel_13060\\2474264056.py\", line 27, in classificar_imagem\n",
      "    model = load_model(model_path)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 196, in load_model\n",
      "    return legacy_h5_format.load_model_from_hdf5(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 116, in load_model_from_hdf5\n",
      "    f = h5py.File(filepath, mode=\"r\")\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 561, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py\\\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'model (1).h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\marcc\\AppData\\Local\\Temp\\ipykernel_13060\\2474264056.py\", line 27, in classificar_imagem\n",
      "    model = load_model(model_path)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 196, in load_model\n",
      "    return legacy_h5_format.load_model_from_hdf5(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 116, in load_model_from_hdf5\n",
      "    f = h5py.File(filepath, mode=\"r\")\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 561, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py\\\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'model (1).h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\marcc\\AppData\\Local\\Temp\\ipykernel_13060\\2474264056.py\", line 27, in classificar_imagem\n",
      "    model = load_model(model_path)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 196, in load_model\n",
      "    return legacy_h5_format.load_model_from_hdf5(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 116, in load_model_from_hdf5\n",
      "    f = h5py.File(filepath, mode=\"r\")\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 561, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py\\\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'model (1).h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\marcc\\AppData\\Local\\Temp\\ipykernel_13060\\2474264056.py\", line 27, in classificar_imagem\n",
      "    model = load_model(model_path)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 196, in load_model\n",
      "    return legacy_h5_format.load_model_from_hdf5(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 116, in load_model_from_hdf5\n",
      "    f = h5py.File(filepath, mode=\"r\")\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 561, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\h5py\\_hl\\files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py\\\\_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py\\\\h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'model (1).h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, Label, Button\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Função para exibir informações de depuração da imagem e retornar uma string com os dados\n",
    "def exibir_informacoes_imagem(img_path, img_array):\n",
    "    info = (\n",
    "        f\"Informações da imagem '{os.path.basename(img_path)}':\\n\"\n",
    "        f\" - Formato: {img_array.shape}\\n\"\n",
    "        f\" - Tipo de dado: {img_array.dtype}\\n\"\n",
    "        f\" - Valor mínimo de pixel: {img_array.min()}\\n\"\n",
    "        f\" - Valor máximo de pixel: {img_array.max()}\\n\"\n",
    "        f\" - Valor médio de pixel: {img_array.mean():.2f}\\n\"\n",
    "    )\n",
    "    print(info)\n",
    "    return info\n",
    "\n",
    "# Função que realiza a classificação e atualiza a interface gráfica\n",
    "def classificar_imagem():\n",
    "    # ===== 1. Carregar o Modelo Treinado =====\n",
    "    model_path = 'models/model (1).h5'  # Ajuste o caminho conforme necessário\n",
    "    model = load_model(model_path)\n",
    "    print(f\"Modelo carregado de: {model_path}\")\n",
    "\n",
    "    # ===== 2. Definir o Mapeamento das Classes =====\n",
    "    class_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'I', 'L', 'M',\n",
    "                    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y']\n",
    "\n",
    "    # ===== 3. Selecionar a Imagem =====\n",
    "    image_path = filedialog.askopenfilename(title=\"Selecione uma imagem\",\n",
    "                                            filetypes=[(\"Imagens\", \"*.png;*.jpg;*.jpeg;*.bmp;*.gif\")])\n",
    "    if not image_path:\n",
    "        print(\"Nenhuma imagem selecionada.\")\n",
    "        return\n",
    "\n",
    "    # Carregar e redimensionar a imagem para o tamanho utilizado no treinamento\n",
    "    img = image.load_img(image_path, target_size=(64, 64))\n",
    "    img_array = image.img_to_array(img)\n",
    "    info = exibir_informacoes_imagem(image_path, img_array)\n",
    "\n",
    "    # Normalizar os pixels e ajustar as dimensões para simular um batch\n",
    "    img_array_normalized = img_array / 255.0\n",
    "    img_array_normalized = np.expand_dims(img_array_normalized, axis=0)\n",
    "\n",
    "    # ===== 4. Fazer a Predição =====\n",
    "    predictions = model.predict(img_array_normalized)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    predicted_class = class_labels[predicted_class_index]\n",
    "    print(f'Classe prevista: {predicted_class}')\n",
    "\n",
    "    # ===== 5. Atualizar a Interface Gráfica =====\n",
    "    # Exibir a imagem na janela usando PIL e ImageTk\n",
    "    pil_img = Image.open(image_path)\n",
    "    pil_img = pil_img.resize((200, 200))  # Redimensiona para visualização na interface\n",
    "    tk_img = ImageTk.PhotoImage(pil_img)\n",
    "    label_img.config(image=tk_img)\n",
    "    label_img.image = tk_img\n",
    "\n",
    "    # Exibir o resultado e as informações da imagem\n",
    "    label_result.config(text=f\"Classe prevista: {predicted_class}\")\n",
    "    label_info.config(text=info)\n",
    "\n",
    "    # ===== 6. Exibir um Gráfico com a Distribuição das Probabilidades =====\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(class_labels)), predictions[0])\n",
    "    plt.xticks(range(len(class_labels)), class_labels)\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.ylabel(\"Probabilidade\")\n",
    "    plt.title(\"Distribuição de Probabilidades\")\n",
    "    plt.show()\n",
    "\n",
    "# Configuração da janela principal do Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"Classificador de Imagens\")\n",
    "\n",
    "# Botão para selecionar e classificar a imagem\n",
    "button_classify = Button(root, text=\"Selecionar e Classificar Imagem\", command=classificar_imagem)\n",
    "button_classify.pack(pady=10)\n",
    "\n",
    "# Label para exibir a imagem selecionada\n",
    "label_img = Label(root)\n",
    "label_img.pack(pady=10)\n",
    "\n",
    "# Label para exibir o resultado da classificação\n",
    "label_result = Label(root, text=\"Classe prevista:\")\n",
    "label_result.pack(pady=5)\n",
    "\n",
    "# Label para exibir informações de depuração da imagem\n",
    "label_info = Label(root, text=\"\", justify=\"left\")\n",
    "label_info.pack(pady=5)\n",
    "\n",
    "# Iniciar a interface gráfica\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado de: models/model (1).h5\n",
      "Informações da imagem '3.png':\n",
      " - Formato: (64, 64, 3)\n",
      " - Tipo de dado: float32\n",
      " - Valor mínimo de pixel: 78.0\n",
      " - Valor máximo de pixel: 255.0\n",
      " - Valor médio de pixel: 214.34\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "Classe prevista: W\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, Label, Button\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Função para exibir informações de depuração da imagem e retornar uma string com os dados\n",
    "def exibir_informacoes_imagem(img_path, img_array):\n",
    "    info = (\n",
    "        f\"Informações da imagem '{os.path.basename(img_path)}':\\n\"\n",
    "        f\" - Formato: {img_array.shape}\\n\"\n",
    "        f\" - Tipo de dado: {img_array.dtype}\\n\"\n",
    "        f\" - Valor mínimo de pixel: {img_array.min()}\\n\"\n",
    "        f\" - Valor máximo de pixel: {img_array.max()}\\n\"\n",
    "        f\" - Valor médio de pixel: {img_array.mean():.2f}\\n\"\n",
    "    )\n",
    "    print(info)\n",
    "    return info\n",
    "\n",
    "# Função que realiza a classificação e atualiza a interface gráfica\n",
    "def classificar_imagem():\n",
    "    # ===== 1. Carregar o Modelo Treinado =====\n",
    "    model_path = 'models/model (1).h5'  # Ajuste o caminho conforme necessário\n",
    "    model = load_model(model_path)\n",
    "    print(f\"Modelo carregado de: {model_path}\")\n",
    "\n",
    "    # ===== 2. Definir o Mapeamento das Classes =====\n",
    "    class_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'I', 'L', 'M',\n",
    "                    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y']\n",
    "\n",
    "    # ===== 3. Selecionar a Imagem =====\n",
    "    image_path = filedialog.askopenfilename(title=\"Selecione uma imagem\",\n",
    "                                            filetypes=[(\"Imagens\", \"*.png;*.jpg;*.jpeg;*.bmp;*.gif\")])\n",
    "    if not image_path:\n",
    "        print(\"Nenhuma imagem selecionada.\")\n",
    "        return\n",
    "\n",
    "    # Carregar e redimensionar a imagem para o tamanho utilizado no treinamento\n",
    "    img = image.load_img(image_path, target_size=(64, 64))\n",
    "    img_array = image.img_to_array(img)\n",
    "    info = exibir_informacoes_imagem(image_path, img_array)\n",
    "\n",
    "    # Normalizar os pixels e ajustar as dimensões para simular um batch\n",
    "    img_array_normalized = img_array / 255.0\n",
    "    img_array_normalized = np.expand_dims(img_array_normalized, axis=0)\n",
    "\n",
    "    # ===== 4. Fazer a Predição =====\n",
    "    predictions = model.predict(img_array_normalized)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    predicted_class = class_labels[predicted_class_index]\n",
    "    print(f'Classe prevista: {predicted_class}')\n",
    "\n",
    "    # ===== 5. Atualizar a Interface Gráfica =====\n",
    "    # Exibir a imagem na janela usando PIL e ImageTk\n",
    "    pil_img = Image.open(image_path)\n",
    "    pil_img = pil_img.resize((200, 200))  # Redimensiona para visualização na interface\n",
    "    tk_img = ImageTk.PhotoImage(pil_img)\n",
    "    label_img.config(image=tk_img)\n",
    "    label_img.image = tk_img\n",
    "\n",
    "    # Exibir o resultado e as informações da imagem\n",
    "    label_result.config(text=f\"Classe prevista: {predicted_class}\")\n",
    "    label_info.config(text=info)\n",
    "\n",
    "    # ===== 6. Exibir um Gráfico com a Distribuição das Probabilidades =====\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(class_labels)), predictions[0])\n",
    "    plt.xticks(range(len(class_labels)), class_labels)\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.ylabel(\"Probabilidade\")\n",
    "    plt.title(\"Distribuição de Probabilidades\")\n",
    "    plt.show()\n",
    "\n",
    "# Configuração da janela principal do Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"Classificador de Imagens\")\n",
    "\n",
    "# Botão para selecionar e classificar a imagem\n",
    "button_classify = Button(root, text=\"Selecionar e Classificar Imagem\", command=classificar_imagem)\n",
    "button_classify.pack(pady=10)\n",
    "\n",
    "# Label para exibir a imagem selecionada\n",
    "label_img = Label(root)\n",
    "label_img.pack(pady=10)\n",
    "\n",
    "# Label para exibir o resultado da classificação\n",
    "label_result = Label(root, text=\"Classe prevista:\")\n",
    "label_result.pack(pady=5)\n",
    "\n",
    "# Label para exibir informações de depuração da imagem\n",
    "label_info = Label(root, text=\"\", justify=\"left\")\n",
    "label_info.pack(pady=5)\n",
    "\n",
    "# Iniciar a interface gráfica\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, Label, Button\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Função para exibir informações de depuração da imagem e retornar uma string com os dados\n",
    "def exibir_informacoes_imagem(img_path, img_array):\n",
    "    info = (\n",
    "        f\"Informações da imagem '{os.path.basename(img_path)}':\\n\"\n",
    "        f\" - Formato: {img_array.shape}\\n\"\n",
    "        f\" - Tipo de dado: {img_array.dtype}\\n\"\n",
    "        f\" - Valor mínimo de pixel: {img_array.min()}\\n\"\n",
    "        f\" - Valor máximo de pixel: {img_array.max()}\\n\"\n",
    "        f\" - Valor médio de pixel: {img_array.mean():.2f}\\n\"\n",
    "    )\n",
    "    print(info)\n",
    "    return info\n",
    "\n",
    "# Função que realiza a classificação e atualiza a interface gráfica\n",
    "def classificar_imagem():\n",
    "    # ===== 1. Carregar o Modelo Treinado =====\n",
    "    model_path = 'models/model (1).h5'  # Ajuste o caminho conforme necessário\n",
    "    model = load_model(model_path)\n",
    "    print(f\"Modelo carregado de: {model_path}\")\n",
    "\n",
    "    # ===== 2. Definir o Mapeamento das Classes =====\n",
    "    class_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'I', 'L', 'M',\n",
    "                    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y']\n",
    "\n",
    "    # ===== 3. Selecionar a Imagem =====\n",
    "    image_path = filedialog.askopenfilename(title=\"Selecione uma imagem\",\n",
    "                                            filetypes=[(\"Imagens\", \"*.png;*.jpg;*.jpeg;*.bmp;*.gif\")])\n",
    "    if not image_path:\n",
    "        print(\"Nenhuma imagem selecionada.\")\n",
    "        return\n",
    "\n",
    "    # Carregar e redimensionar a imagem para o tamanho utilizado no treinamento\n",
    "    img = image.load_img(image_path, target_size=(64, 64))\n",
    "    img_array = image.img_to_array(img)\n",
    "    info = exibir_informacoes_imagem(image_path, img_array)\n",
    "\n",
    "    # Normalizar os pixels e ajustar as dimensões para simular um batch\n",
    "    img_array_normalized = img_array / 255.0\n",
    "    img_array_normalized = np.expand_dims(img_array_normalized, axis=0)\n",
    "\n",
    "    # ===== 4. Fazer a Predição =====\n",
    "    predictions = model.predict(img_array_normalized)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    predicted_class = class_labels[predicted_class_index]\n",
    "    print(f'Classe prevista: {predicted_class}')\n",
    "\n",
    "    # ===== 5. Atualizar a Interface Gráfica =====\n",
    "    # Exibir a imagem na janela usando PIL e ImageTk\n",
    "    pil_img = Image.open(image_path)\n",
    "    pil_img = pil_img.resize((200, 200))  # Redimensiona para visualização na interface\n",
    "    tk_img = ImageTk.PhotoImage(pil_img)\n",
    "    label_img.config(image=tk_img)\n",
    "    label_img.image = tk_img\n",
    "\n",
    "    # Exibir o resultado e as informações da imagem\n",
    "    label_result.config(text=f\"Classe prevista: {predicted_class}\")\n",
    "    label_info.config(text=info)\n",
    "\n",
    "    # ===== 6. Exibir um Gráfico com a Distribuição das Probabilidades =====\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(class_labels)), predictions[0])\n",
    "    plt.xticks(range(len(class_labels)), class_labels)\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.ylabel(\"Probabilidade\")\n",
    "    plt.title(\"Distribuição de Probabilidades\")\n",
    "    plt.show()\n",
    "\n",
    "# Configuração da janela principal do Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"Classificador de Imagens\")\n",
    "\n",
    "# Botão para selecionar e classificar a imagem\n",
    "button_classify = Button(root, text=\"Selecionar e Classificar Imagem\", command=classificar_imagem)\n",
    "button_classify.pack(pady=10)\n",
    "\n",
    "# Label para exibir a imagem selecionada\n",
    "label_img = Label(root)\n",
    "label_img.pack(pady=10)\n",
    "\n",
    "# Label para exibir o resultado da classificação\n",
    "label_result = Label(root, text=\"Classe prevista:\")\n",
    "label_result.pack(pady=5)\n",
    "\n",
    "# Label para exibir informações de depuração da imagem\n",
    "label_info = Label(root, text=\"\", justify=\"left\")\n",
    "label_info.pack(pady=5)\n",
    "\n",
    "# Iniciar a interface gráfica\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado de: models/model (1).h5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import Label, Button\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# ===== 1. Carregar o Modelo Treinado =====\n",
    "model_path = 'models/model (1).h5'  # Ajuste o caminho conforme necessário\n",
    "model = load_model(model_path, compile=False)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(f\"Modelo carregado de: {model_path}\")\n",
    "\n",
    "# ===== 2. Definir o Mapeamento das Classes =====\n",
    "class_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'I', 'L', 'M',\n",
    "                'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y']\n",
    "\n",
    "# ===== 3. Configurar a Interface Tkinter e a Captura de Vídeo =====\n",
    "root = tk.Tk()\n",
    "root.title(\"Detecção e Classificação de Mão\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "current_frame = None\n",
    "\n",
    "last_bounding_box = None  # (x_min, y_min, x_max, y_max)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=1,\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)\n",
    "\n",
    "video_label = Label(root)\n",
    "video_label.pack()\n",
    "\n",
    "capture_button = Button(root, text=\"Tirar Foto\", command=lambda: capture_photo(), state=tk.DISABLED)\n",
    "capture_button.pack(pady=10)\n",
    "\n",
    "captured_label = Label(root)\n",
    "captured_label.pack(pady=10)\n",
    "\n",
    "label_result = Label(root, text=\"Classe prevista:\")\n",
    "label_result.pack(pady=5)\n",
    "\n",
    "def update_frame():\n",
    "    global current_frame, last_bounding_box\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        current_frame = frame.copy()\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(frame_rgb)\n",
    "        hand_detected = False\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_detected = True\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                h, w, _ = frame.shape\n",
    "                x_coords = [int(lm.x * w) for lm in hand_landmarks.landmark]\n",
    "                y_coords = [int(lm.y * h) for lm in hand_landmarks.landmark]\n",
    "\n",
    "                # Calcular a caixa delimitadora inicial\n",
    "                x_min_orig, x_max_orig = min(x_coords), max(x_coords)\n",
    "                y_min_orig, y_max_orig = min(y_coords), max(y_coords)\n",
    "                hand_width = x_max_orig - x_min_orig\n",
    "                hand_height = y_max_orig - y_min_orig\n",
    "                size = max(hand_width, hand_height)\n",
    "\n",
    "                # Centralizar a caixa delimitadora\n",
    "                center_x = (x_min_orig + x_max_orig) // 2\n",
    "                center_y = (y_min_orig + y_max_orig) // 2\n",
    "\n",
    "                # Definir coordenadas do quadrado\n",
    "                x_min = center_x - (size // 2)\n",
    "                x_max = center_x + (size // 2)\n",
    "                y_min = center_y - (size // 2)\n",
    "                y_max = center_y + (size // 2)\n",
    "\n",
    "                # Adicionar padding dinâmico (20% do tamanho)\n",
    "                padding = int(size * 0.2)\n",
    "                x_min = max(x_min - padding, 0)\n",
    "                y_min = max(y_min - padding, 0)\n",
    "                x_max = min(x_max + padding, w)\n",
    "                y_max = min(y_max + padding, h)\n",
    "\n",
    "                # Atualizar last_bounding_box\n",
    "                last_bounding_box = (x_min, y_min, x_max, y_max)\n",
    "\n",
    "                # Desenhar retângulo\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "        capture_button.config(state=tk.NORMAL if hand_detected else tk.DISABLED)\n",
    "        img_tk = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        imgtk = ImageTk.PhotoImage(image=img_tk)\n",
    "        video_label.imgtk = imgtk\n",
    "        video_label.configure(image=imgtk)\n",
    "\n",
    "    video_label.after(10, update_frame)\n",
    "\n",
    "def preprocess_image(pil_img):\n",
    "    pil_img = pil_img.convert(\"RGB\")\n",
    "    pil_img = pil_img.resize((64, 64))\n",
    "    img_array = image.img_to_array(pil_img)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def capture_photo():\n",
    "    global current_frame, last_bounding_box\n",
    "    if current_frame is not None and last_bounding_box is not None:\n",
    "        x_min, y_min, x_max, y_max = last_bounding_box\n",
    "        cropped_frame = current_frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        # Salvar e exibir a imagem recortada\n",
    "        cv2.imwrite(\"captured_hand.jpg\", cropped_frame)\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2RGB))\n",
    "        pil_img_display = pil_img.resize((200, 200))\n",
    "        tk_img = ImageTk.PhotoImage(image=pil_img_display)\n",
    "        captured_label.config(image=tk_img)\n",
    "        captured_label.image = tk_img\n",
    "\n",
    "        # Classificar a imagem\n",
    "        img_array = preprocess_image(pil_img)\n",
    "        predictions = model.predict(img_array)\n",
    "        predicted_class = class_labels[np.argmax(predictions)]\n",
    "        label_result.config(text=f\"Classe prevista: {predicted_class}\")\n",
    "\n",
    "update_frame()\n",
    "root.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado de: models/model (1).h5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 234, in from_config\n",
      "    return cls(**config)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\layers\\convolutional\\depthwise_conv2d.py\", line 120, in __init__\n",
      "    super().__init__(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_depthwise_conv.py\", line 106, in __init__\n",
      "    super().__init__(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 287, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized keyword arguments passed to DepthwiseConv2D: {'groups': 1}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\marcc\\AppData\\Local\\Temp\\ipykernel_18452\\1652605399.py\", line 27, in classificar_imagem\n",
      "    model = load_model(model_path)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 196, in load_model\n",
      "    return legacy_h5_format.load_model_from_hdf5(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 133, in load_model_from_hdf5\n",
      "    model = saving_utils.model_from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 85, in model_from_config\n",
      "    return serialization.deserialize_keras_object(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py\", line 495, in deserialize_keras_object\n",
      "    deserialized_obj = cls.from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\models\\sequential.py\", line 350, in from_config\n",
      "    layer = saving_utils.model_from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 85, in model_from_config\n",
      "    return serialization.deserialize_keras_object(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py\", line 495, in deserialize_keras_object\n",
      "    deserialized_obj = cls.from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\models\\sequential.py\", line 350, in from_config\n",
      "    layer = saving_utils.model_from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 85, in model_from_config\n",
      "    return serialization.deserialize_keras_object(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py\", line 495, in deserialize_keras_object\n",
      "    deserialized_obj = cls.from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\models\\model.py\", line 582, in from_config\n",
      "    return functional_from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 551, in functional_from_config\n",
      "    process_layer(layer_data)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 519, in process_layer\n",
      "    layer = saving_utils.model_from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 85, in model_from_config\n",
      "    return serialization.deserialize_keras_object(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py\", line 504, in deserialize_keras_object\n",
      "    deserialized_obj = cls.from_config(cls_config)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 236, in from_config\n",
      "    raise TypeError(\n",
      "TypeError: Error when deserializing class 'DepthwiseConv2D' using config={'name': 'expanded_conv_depthwise', 'trainable': True, 'dtype': 'float32', 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': False, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'depthwise_regularizer': None, 'depthwise_constraint': None}.\n",
      "\n",
      "Exception encountered: Unrecognized keyword arguments passed to DepthwiseConv2D: {'groups': 1}\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 234, in from_config\n",
      "    return cls(**config)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\layers\\convolutional\\depthwise_conv2d.py\", line 120, in __init__\n",
      "    super().__init__(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_depthwise_conv.py\", line 106, in __init__\n",
      "    super().__init__(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 287, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized keyword arguments passed to DepthwiseConv2D: {'groups': 1}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\marcc\\AppData\\Local\\Temp\\ipykernel_18452\\1652605399.py\", line 27, in classificar_imagem\n",
      "    model = load_model(model_path)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 196, in load_model\n",
      "    return legacy_h5_format.load_model_from_hdf5(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 133, in load_model_from_hdf5\n",
      "    model = saving_utils.model_from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 85, in model_from_config\n",
      "    return serialization.deserialize_keras_object(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py\", line 495, in deserialize_keras_object\n",
      "    deserialized_obj = cls.from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\models\\sequential.py\", line 350, in from_config\n",
      "    layer = saving_utils.model_from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 85, in model_from_config\n",
      "    return serialization.deserialize_keras_object(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py\", line 495, in deserialize_keras_object\n",
      "    deserialized_obj = cls.from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\models\\sequential.py\", line 350, in from_config\n",
      "    layer = saving_utils.model_from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 85, in model_from_config\n",
      "    return serialization.deserialize_keras_object(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py\", line 495, in deserialize_keras_object\n",
      "    deserialized_obj = cls.from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\models\\model.py\", line 582, in from_config\n",
      "    return functional_from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 551, in functional_from_config\n",
      "    process_layer(layer_data)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 519, in process_layer\n",
      "    layer = saving_utils.model_from_config(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 85, in model_from_config\n",
      "    return serialization.deserialize_keras_object(\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py\", line 504, in deserialize_keras_object\n",
      "    deserialized_obj = cls.from_config(cls_config)\n",
      "  File \"c:\\Users\\marcc\\anaconda3\\envs\\trabalho_libras_uninter\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 236, in from_config\n",
      "    raise TypeError(\n",
      "TypeError: Error when deserializing class 'DepthwiseConv2D' using config={'name': 'expanded_conv_depthwise', 'trainable': True, 'dtype': 'float32', 'kernel_size': [3, 3], 'strides': [1, 1], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'linear', 'use_bias': False, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'bias_regularizer': None, 'activity_regularizer': None, 'bias_constraint': None, 'depth_multiplier': 1, 'depthwise_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'depthwise_regularizer': None, 'depthwise_constraint': None}.\n",
      "\n",
      "Exception encountered: Unrecognized keyword arguments passed to DepthwiseConv2D: {'groups': 1}\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import Label, Button\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# ===== 1. Carregar o Modelo Treinado =====\n",
    "model_path = 'models/model (1).h5'  # Ajuste o caminho conforme necessário\n",
    "model = load_model(model_path, compile=False)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(f\"Modelo carregado de: {model_path}\")\n",
    "\n",
    "# ===== 2. Definir o Mapeamento das Classes =====\n",
    "class_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'I', 'L', 'M',\n",
    "                'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y']\n",
    "\n",
    "# Variáveis globais para armazenar o frame atual e a caixa delimitadora da mão\n",
    "current_frame = None\n",
    "last_box = None  # (x_min, y_min, x_max, y_max)\n",
    "\n",
    "# ===== 3. Configurar a Interface Tkinter e a Captura de Vídeo =====\n",
    "root = tk.Tk()\n",
    "root.title(\"Detecção e Classificação de Mão\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=1,\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)\n",
    "\n",
    "video_label = Label(root)\n",
    "video_label.pack()\n",
    "\n",
    "capture_button = Button(root, text=\"Tirar Foto\", command=lambda: capture_photo(), state=tk.DISABLED)\n",
    "capture_button.pack(pady=10)\n",
    "\n",
    "captured_label = Label(root)\n",
    "captured_label.pack(pady=10)\n",
    "\n",
    "label_result = Label(root, text=\"Classe prevista:\")\n",
    "label_result.pack(pady=5)\n",
    "\n",
    "def update_frame():\n",
    "    global current_frame, last_box\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame = cv2.flip(frame, 1)  # Espelha a imagem\n",
    "        current_frame = frame.copy()\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(frame_rgb)\n",
    "        hand_detected = False\n",
    "        last_box = None  # Reinicia a cada frame\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_detected = True\n",
    "            # Considera apenas a primeira mão detectada\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            h, w, _ = frame.shape\n",
    "            points = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                x_pixel = int(lm.x * w)\n",
    "                y_pixel = int(lm.y * h)\n",
    "                points.append((x_pixel, y_pixel))\n",
    "            # Calcula o retângulo delimitador (bounding box) dos landmarks\n",
    "            x, y, box_w, box_h = cv2.boundingRect(np.array(points, dtype=np.int32))\n",
    "            # Ajusta o retângulo com um fator de escala (por exemplo, 1.3)\n",
    "            scale_factor = 1.3\n",
    "            center_x = x + box_w / 2\n",
    "            center_y = y + box_h / 2\n",
    "            new_w = box_w * scale_factor\n",
    "            new_h = box_h * scale_factor\n",
    "            new_x = int(center_x - new_w / 2)\n",
    "            new_y = int(center_y - new_h / 2)\n",
    "            # Garante que o retângulo esteja dentro dos limites da imagem\n",
    "            new_x = max(new_x, 0)\n",
    "            new_y = max(new_y, 0)\n",
    "            new_x2 = min(new_x + int(new_w), frame.shape[1])\n",
    "            new_y2 = min(new_y + int(new_h), frame.shape[0])\n",
    "            last_box = (new_x, new_y, new_x2, new_y2)\n",
    "            # Desenha o retângulo na imagem\n",
    "            cv2.rectangle(frame, (new_x, new_y), (new_x2, new_y2), (0, 255, 0), 2)\n",
    "\n",
    "        capture_button.config(state=tk.NORMAL if hand_detected else tk.DISABLED)\n",
    "        img_tk = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        imgtk = ImageTk.PhotoImage(image=img_tk)\n",
    "        video_label.imgtk = imgtk\n",
    "        video_label.configure(image=imgtk)\n",
    "\n",
    "    video_label.after(10, update_frame)\n",
    "\n",
    "def preprocess_image(pil_img):\n",
    "    pil_img = pil_img.convert(\"RGB\")\n",
    "    pil_img = pil_img.resize((64, 64))\n",
    "    img_array = image.img_to_array(pil_img)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def capture_photo():\n",
    "    global current_frame, last_box\n",
    "    if current_frame is not None and last_box is not None:\n",
    "        x_min, y_min, x_max, y_max = last_box\n",
    "        # Adiciona um padding extra (se desejar)\n",
    "        padding = 20\n",
    "        x_min = max(x_min - padding, 0)\n",
    "        y_min = max(y_min - padding, 0)\n",
    "        x_max = min(x_max + padding, current_frame.shape[1])\n",
    "        y_max = min(y_max + padding, current_frame.shape[0])\n",
    "        \n",
    "        # Aplica desfoque ao fundo na imagem inteira\n",
    "        full_blurred = cv2.GaussianBlur(current_frame, (21, 21), 0)\n",
    "        result_full = full_blurred.copy()\n",
    "        # Mantém a região do retângulo (a mão) nítida\n",
    "        result_full[y_min:y_max, x_min:x_max] = current_frame[y_min:y_max, x_min:x_max]\n",
    "        # Em vez de criar outra imagem, reescreve (atualiza) a mesma imagem na interface\n",
    "        result = result_full[y_min:y_max, x_min:x_max].copy()\n",
    "\n",
    "        # Atualiza a imagem exibida na interface (sem salvar em disco)\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "        pil_img_display = pil_img.resize((200, 200))\n",
    "        tk_img = ImageTk.PhotoImage(image=pil_img_display)\n",
    "        captured_label.config(image=tk_img)\n",
    "        captured_label.image = tk_img\n",
    "\n",
    "        # Pré-processa a imagem para classificação\n",
    "        img_array = preprocess_image(pil_img)\n",
    "        predictions = model.predict(img_array)\n",
    "        predicted_class = class_labels[np.argmax(predictions)]\n",
    "        label_result.config(text=f\"Classe prevista: {predicted_class}\")\n",
    "\n",
    "update_frame()\n",
    "root.mainloop()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado de: models/model (1).h5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trabalho_libras_uninter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
